{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass, os, requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "astra_db_api_endpoint = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "astra_db_application_token = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "\n",
    "if \"ASTRA_DB_API_ENDPOINT\" not in os.environ:\n",
    "  os.environ[\"ASTRA_DB_API_ENDPOINT\"] = getpass.getpass(\"Provide your Astra DB Endpoint\")\n",
    "\n",
    "if \"ASTRA_DB_APPLICATION_TOKEN\" not in os.environ:\n",
    "  os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass.getpass(\"Provide your Astra DB Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astra vector store configured\n"
     ]
    }
   ],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "\n",
    "# Configure your embedding model and vector store\n",
    "embedding = OpenAIEmbeddings()\n",
    "vstore = AstraDBVectorStore(\n",
    "    collection_name=\"test\",\n",
    "    embedding=embedding,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    ")\n",
    "print(\"Astra vector store configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uploaded_file in os.listdir(\"uploaded_docs\"):\n",
    "    file_path = os.path.join(\"uploaded_docs\", uploaded_file)\n",
    "    # # Save the uploaded file to disk\n",
    "    # file_path = os.path.join(\"uploaded_docs\", uploaded_file.name)\n",
    "    # with open(file_path, \"wb\") as f:\n",
    "    #     f.write(uploaded_file.getbuffer())\n",
    "\n",
    "\n",
    "    all_chunks = []  \n",
    "\n",
    "    chunks = partition_pdf(\n",
    "        filename=file_path,\n",
    "        infer_table_structure=True,            # extract tables\n",
    "        strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "        extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "        # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "        extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "        chunking_strategy=\"by_title\",          # or 'basic'\n",
    "        max_characters=10000,                  # defaults to 500\n",
    "        combine_text_under_n_chars=2000,       # defaults to 0\n",
    "        new_after_n_chars=6000,\n",
    "\n",
    "        # extract_images_in_pdf=True,          # deprecated\n",
    "    )\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)\n",
    "\n",
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(all_chunks)\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatGroq(temperature=0.5, model=\"llama-3.1-8b-instant\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Summarize text\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})\n",
    "\n",
    "# Summarize tables\n",
    "tables_html = [table.metadata.text_as_html for table in tables]\n",
    "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})\n",
    "\n",
    "\n",
    "# Summarise Images\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                the image is part of a research paper explaining the transformers\n",
    "                architecture. Be specific about graphs, such as bar plots.\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "image_summaries = chain.batch(images)\n",
    "\n",
    "\n",
    "# id_key = \"undefined\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={\"doctype\":\"text\"}) for _, summary in enumerate(text_summaries)\n",
    "]\n",
    "if summary_texts:\n",
    "    inserted_ids1 = vstore.add_documents(summary_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={\"doctype\":\"table\"}) for _, summary in enumerate(table_summaries)\n",
    "]\n",
    "if summary_tables:\n",
    "    inserted_ids2 = vstore.add_documents(summary_tables)\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={\"doctype\":\"image\"}) for _, summary in enumerate(image_summaries)\n",
    "]\n",
    "if summary_img:\n",
    "    inserted_ids3 = vstore.add_documents(summary_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prati\\AppData\\Local\\Temp\\ipykernel_2252\\4075439123.py:15: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Pratik Bhangale's skills include proficiency in various programming languages and tools, with expertise in areas such as NLP, deep learning, and machine learning.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retriever = vstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "chain.invoke(\"give 5 skills of pratik bhangale?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Proficiency in various programming languages\\n2. Expertise in areas such as NLP, deep learning, and machine learning\\n3. Experience as a data science intern\\n4. Worked on projects involving generative AI-driven medical imaging\\n5. Worked on projects involving SAP installation search automation using agentic AI and multi-modal RAG'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"give 5 skills of pratik bhangale?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Pratik Bhangale is a master's student in Artificial Intelligence at Rochester Institute of Technology, with a strong background in computer science and engineering. His technical skills include proficiency in various programming languages and tools, with expertise in areas such as NLP, deep learning, and machine learning. He has experience as a data science intern and has worked on several projects, including generative AI-driven medical imaging and SAP installation search automation using agentic AI and multi-modal RAG.' metadata={'doctype': 'text'}\n",
      "page_content='Engineered model optimization techniques like knowledge distillation and pruning achieved an 87.5% reduction in model size while maintaining performance. A data preprocessing pipeline handled 200,000 conversation records improving data quality. A neural network model achieved 97% accuracy, and its size was reduced from 150MB to 40MB for edge devices. An Android app was developed for the model's deployment.' metadata={'doctype': 'text'}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print the content of the documents\n",
    "retrieved_docs = retriever.invoke(\"give 5 skills of pratik bhangale?\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
